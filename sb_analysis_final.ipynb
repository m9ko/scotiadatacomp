{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "trainFileName = 'train_ScotiaDSD.csv'\n",
    "testFileName = 'test_ScotiaDSD.csv'\n",
    "data = pd.read_csv(os.path.join(os.getcwd(), trainFileName))\n",
    "evalData = pd.read_csv(os.path.join(os.getcwd(), testFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARY_COLUMNS (not including FRAUD_FLAG), STRING_COLUMNS, NUMERICAL_COLUMNS\n",
    "BINARY_COLUMNS = [\n",
    " 'CARD_NOT_PRESENT',\n",
    " 'FLAG_LX',\n",
    " 'FLAG_ATM',\n",
    " 'FLAG_AUTO',\n",
    " 'FLAG_CASH',\n",
    " 'FLAG_LS',\n",
    " 'FLAG_DISCOUNT',\n",
    " 'FLAG_RECREA',\n",
    " 'FLAG_ELCTRNCS',\n",
    " 'FLAG_REG_AMT',\n",
    " 'FLAG_FASTFOOD',\n",
    " 'FLAG_GAS',\n",
    " 'FLAG_HIGH_AMT',\n",
    " 'FLAG_HIGH_RECREA',\n",
    " 'FLAG_INTERNET',\n",
    " 'FLAG_INTERNATIONAL',\n",
    " 'FLAG_JEWELRY',\n",
    " 'FLAG_LOW_AMT',\n",
    " 'FLAG_MANUAL_ENTRY',\n",
    " 'FLAG_PHONE_ORDER',\n",
    " 'FLAG_PURCHASE_EXCLUDING_GAS',\n",
    " 'FLAG_PLANNED',\n",
    " 'FLAG_RISKY',\n",
    " 'FLAG_SWIPE',\n",
    " 'FLAG_TRAVEL_ONLY',\n",
    " 'FLAG_TRAVEL_AND_ENTERTAINMENT',\n",
    " 'FLAG_WEEKEND']\n",
    "\n",
    "STRING_COLUMNS = ['TRANSACTION_ID',\n",
    "'USER_AGENT',\n",
    "'CITY',\n",
    "'EVENT_TIME']\n",
    "\n",
    "NUMERICAL_COLUMNS = ['EVENT_MONTH',\n",
    " 'EVENT_DAY_OF_WEEK',\n",
    " 'AVAIL_CRDT',\n",
    " 'AMOUNT',\n",
    " 'CREDIT_LIMIT',\n",
    " 'MEAN_AUTO_PAST_7DAY',\n",
    " 'MEAN_LS_PAST_7DAY',\n",
    " 'MEAN_RECREA_PAST_7DAY',\n",
    " 'MEAN_REG_AMT_PAST_7DAY',\n",
    " 'MEAN_FASTFOOD_PAST_7DAY',\n",
    " 'MEAN_HIGH_AMT_PAST_7DAY',\n",
    " 'MEAN_HIGH_RECREA_PAST_7DAY',\n",
    " 'MEAN_INTERNET_PAST_7DAY',\n",
    " 'MEAN_INTERNATIONAL_PAST_7DAY',\n",
    " 'MEAN_JEWELRY_PAST_7DAY',\n",
    " 'MEAN_LOW_AMT_PAST_7DAY',\n",
    " 'MEAN_MANUAL_ENTRY_PAST_7DAY',\n",
    " 'MEAN_PHONE_ORDER_PAST_7DAY',\n",
    " 'MEAN_PLANNED_PAST_7DAY',\n",
    " 'MEAN_SWIPE_PAST_7DAY',\n",
    " 'MEAN_TRAVEL_AND_ENTERTAINMENT_PAST_7DAY',\n",
    " 'MEAN_WEEKEND_PAST_7DAY',\n",
    " 'MAX_CASH_PAST_7DAY',\n",
    " 'MAX_LS_PAST_7DAY',\n",
    " 'MAX_RECREA_PAST_7DAY',\n",
    " 'MAX_HIGH_AMT_PAST_7DAY',\n",
    " 'MAX_HIGH_RECREA_PAST_7DAY',\n",
    " 'MAX_INTERNET_PAST_7DAY',\n",
    " 'MAX_PHONE_ORDER_PAST_7DAY',\n",
    " 'MAX_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'MAX_SWIPE_PAST_7DAY',\n",
    " 'MAX_WEEKEND_PAST_7DAY',\n",
    " 'STD_LX_PAST_7DAY',\n",
    " 'STD_FASTFOOD_PAST_7DAY',\n",
    " 'STD_HIGH_AMT_PAST_7DAY',\n",
    " 'STD_INTERNET_PAST_7DAY',\n",
    " 'STD_LOW_AMT_PAST_7DAY',\n",
    " 'STD_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'STD_SWIPE_PAST_7DAY',\n",
    " 'STD_TRAVEL_AND_ENTERTAINMENT_PAST_7DAY',\n",
    " 'SUM_LX_PAST_7DAY',\n",
    " 'SUM_AUTO_PAST_7DAY',\n",
    " 'SUM_LS_PAST_7DAY',\n",
    " 'SUM_RECREA_PAST_7DAY',\n",
    " 'SUM_GAS_PAST_7DAY',\n",
    " 'SUM_HIGH_AMT_PAST_7DAY',\n",
    " 'SUM_INTERNET_PAST_7DAY',\n",
    " 'SUM_INTERNATIONAL_PAST_7DAY',\n",
    " 'SUM_LOW_AMT_PAST_7DAY',\n",
    " 'SUM_MANUAL_ENTRY_PAST_7DAY',\n",
    " 'SUM_PHONE_ORDER_PAST_7DAY',\n",
    " 'SUM_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'SUM_PARTIAL_PAST_7DAY',\n",
    " 'SUM_PLANNED_PAST_7DAY',\n",
    " 'SUM_SWIPE_PAST_7DAY',\n",
    " 'SUM_WEEKEND_PAST_7DAY',\n",
    " 'COUNT_AUTO_PAST_7DAY',\n",
    " 'COUNT_ELCTRNCS_PAST_7DAY',\n",
    " 'COUNT_GAS_PAST_7DAY',\n",
    " 'COUNT_HIGH_AMT_PAST_7DAY',\n",
    " 'COUNT_INTERNET_PAST_7DAY',\n",
    " 'COUNT_LOW_AMT_PAST_7DAY',\n",
    " 'COUNT_MANUAL_ENTRY_PAST_7DAY',\n",
    " 'COUNT_PHONE_ORDER_PAST_7DAY',\n",
    " 'COUNT_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'COUNT_SWIPE_PAST_7DAY',\n",
    " 'COUNT_TRAVEL_AND_ENTERTAINMENT_PAST_7DAY',\n",
    " 'COUNT_WEEKEND_PAST_7DAY',\n",
    " 'MEAN_AUTO_PAST_30DAY',\n",
    " 'MEAN_DISCOUNT_PAST_30DAY',\n",
    " 'MEAN_RECREA_PAST_30DAY',\n",
    " 'MEAN_ELCTRNCS_PAST_30DAY',\n",
    " 'MEAN_REG_AMT_PAST_30DAY',\n",
    " 'MEAN_HIGH_AMT_PAST_30DAY',\n",
    " 'MEAN_INTERNET_PAST_30DAY',\n",
    " 'MEAN_LOW_AMT_PAST_30DAY',\n",
    " 'MEAN_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'MEAN_PHONE_ORDER_PAST_30DAY',\n",
    " 'MEAN_PURCHASE_EXCLUDING_GAS_PAST_30DAY',\n",
    " 'MEAN_PLANNED_PAST_30DAY',\n",
    " 'MEAN_SWIPE_PAST_30DAY',\n",
    " 'MEAN_TRAVEL_AND_ENTERTAINMENT_PAST_30DAY',\n",
    " 'MEAN_WEEKEND_PAST_30DAY',\n",
    " 'MAX_AUTO_PAST_30DAY',\n",
    " 'MAX_LS_PAST_30DAY',\n",
    " 'MAX_ELCTRNCS_PAST_30DAY',\n",
    " 'MAX_FASTFOOD_PAST_30DAY',\n",
    " 'MAX_HIGH_RECREA_PAST_30DAY',\n",
    " 'MAX_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'MAX_PHONE_ORDER_PAST_30DAY',\n",
    " 'MAX_PARTIAL_PAST_30DAY',\n",
    " 'MAX_RISKY_PAST_30DAY',\n",
    " 'MAX_WEEKEND_PAST_30DAY',\n",
    " 'STD_AUTO_PAST_30DAY',\n",
    " 'STD_LS_PAST_30DAY',\n",
    " 'STD_RECREA_PAST_30DAY',\n",
    " 'STD_ELCTRNCS_PAST_30DAY',\n",
    " 'STD_REG_AMT_PAST_30DAY',\n",
    " 'STD_HIGH_RECREA_PAST_30DAY',\n",
    " 'STD_INTERNET_PAST_30DAY',\n",
    " 'STD_LOW_AMT_PAST_30DAY',\n",
    " 'STD_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'STD_PHONE_ORDER_PAST_30DAY',\n",
    " 'STD_PARTIAL_PAST_30DAY',\n",
    " 'STD_SWIPE_PAST_30DAY',\n",
    " 'STD_TRAVEL_ONLY_PAST_30DAY',\n",
    " 'STD_TRAVEL_AND_ENTERTAINMENT_PAST_30DAY',\n",
    " 'SUM_AUTO_PAST_30DAY',\n",
    " 'SUM_LS_PAST_30DAY',\n",
    " 'SUM_DISCOUNT_PAST_30DAY',\n",
    " 'SUM_RECREA_PAST_30DAY',\n",
    " 'SUM_ELCTRNCS_PAST_30DAY',\n",
    " 'SUM_REG_AMT_PAST_30DAY',\n",
    " 'SUM_FASTFOOD_PAST_30DAY',\n",
    " 'SUM_GAS_PAST_30DAY',\n",
    " 'SUM_HIGH_AMT_PAST_30DAY',\n",
    " 'SUM_HIGH_RECREA_PAST_30DAY',\n",
    " 'SUM_INTERNET_PAST_30DAY',\n",
    " 'SUM_INTERNATIONAL_PAST_30DAY',\n",
    " 'SUM_LOW_AMT_PAST_30DAY',\n",
    " 'SUM_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'SUM_PHONE_ORDER_PAST_30DAY',\n",
    " 'SUM_PURCHASE_EXCLUDING_GAS_PAST_30DAY',\n",
    " 'SUM_SWIPE_PAST_30DAY',\n",
    " 'SUM_TRAVEL_ONLY_PAST_30DAY',\n",
    " 'SUM_WEEKEND_PAST_30DAY',\n",
    " 'COUNT_AUTO_PAST_30DAY',\n",
    " 'COUNT_RECREA_PAST_30DAY',\n",
    " 'COUNT_REG_AMT_PAST_30DAY',\n",
    " 'COUNT_FASTFOOD_PAST_30DAY',\n",
    " 'COUNT_GAS_PAST_30DAY',\n",
    " 'COUNT_HIGH_AMT_PAST_30DAY',\n",
    " 'COUNT_INTERNET_PAST_30DAY',\n",
    " 'COUNT_LOW_AMT_PAST_30DAY',\n",
    " 'COUNT_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'COUNT_PHONE_ORDER_PAST_30DAY',\n",
    " 'COUNT_PURCHASE_EXCLUDING_GAS_PAST_30DAY',\n",
    " 'COUNT_PLANNED_PAST_30DAY',\n",
    " 'COUNT_SWIPE_PAST_30DAY',\n",
    " 'COUNT_TRAVEL_AND_ENTERTAINMENT_PAST_30DAY',\n",
    " 'COUNT_WEEKEND_PAST_30DAY',\n",
    " 'PREV_M_INFLATION',\n",
    " 'PREV_M_UNEMP_RATE']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "### Takeaway\n",
    "After having a deep-dive into the dataset, the follwoing conclusions can be made:\n",
    "- there are three distinct data types:\n",
    "    - float (numerical)\n",
    "    - int (binary)\n",
    "    - object (string)\n",
    "- the magnitude of the variables differ greatly\n",
    "- imbalance of target data (97.5% to 2.5%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataframe):\n",
    "    \"\"\" Pre-process the data.\n",
    "            - convert dtypes\n",
    "            - change units to something more useful\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: same pandas Dataframe but preprocessed\n",
    "    \"\"\"\n",
    "    # Convert binary variables into integers\n",
    "    \n",
    "    nonBinaryColumnNamesList = []\n",
    "    for columnName in dataframe:\n",
    "        # Binary variables only have 2 unique values\n",
    "        if len(dataframe[columnName].unique()) == 2:\n",
    "            dataframe[columnName] = dataframe[columnName].astype('int64')\n",
    "        elif (dataframe[columnName].dtype == 'int64') or (dataframe[columnName].dtype == 'float64'):\n",
    "            dataframe[columnName] = dataframe[columnName].astype('float64')\n",
    "\n",
    "    # Convert EVENT_TIME string into seconds to be useful\n",
    "    dataframe['EVENT_TIME'] = dataframe['EVENT_TIME'].apply(lambda x: x[:-1] if x[-1] == ':' else x)\n",
    "    dateTransactionTime = pd.DataFrame(dataframe['EVENT_TIME'].str.split(r':').to_list(), columns=['hour', 'minutes'])\n",
    "    dateTransactionTime = dateTransactionTime.astype('float64')\n",
    "\n",
    "    dataTransactionTimeSeconds = dateTransactionTime['hour'] * 60 * 60 + dateTransactionTime['minutes'] * 60\n",
    "    \n",
    "    # Drop the useless columns now\n",
    "    dataframe = dataframe.drop(STRING_COLUMNS, axis = 1)\n",
    "    dataframe['EVENT_TIME_IN_SECONDS'] = dataTransactionTimeSeconds\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPreprocessed = data_preprocessing(data)\n",
    "NUMERICAL_COLUMNS.append('EVENT_TIME_IN_SECONDS')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataPreprocessed['FRAUD_FLAG']\n",
    "X = dataPreprocessed.drop(['FRAUD_FLAG'], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply RF on a subset of data for efficiency\n",
    "xRF, _, yRF, _ = train_test_split(X, y, train_size=0.1, stratify=y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "rf.fit(xRF, yRF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort = rf.feature_importances_.argsort()\n",
    "featureImportanceColumns = xRF.columns[sort]\n",
    "featureImportanceValues = rf.feature_importances_[sort]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(featureImportanceColumns, featureImportanceValues)\n",
    "plt.xlabel(\"Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportanceThreshold = 0.005\n",
    "\n",
    "keepColumns = featureImportanceColumns[featureImportanceValues >= featureImportanceThreshold]\n",
    "\n",
    "xImportantColumns = X[keepColumns]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the minority class will be sampled until you have 10% of the majority class\n",
    "over = SMOTE(sampling_strategy=0.1)\n",
    "# the majority class will be reduced until you have 50% more than the minority class\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "\n",
    "xSMOTE, ySMOTE = over.fit_resample(xImportantColumns, y)\n",
    "print(f\"Original: \\n{y.value_counts()}\")\n",
    "print(f\"New: \\n{ySMOTE.value_counts()}\")\n",
    "xUnder, yUnder = under.fit_resample(xSMOTE, ySMOTE)\n",
    "print(f\"Original: \\n{ySMOTE.value_counts()}\")\n",
    "print(f\"New: \\n{yUnder.value_counts()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, y_true, y_pred):\n",
    "    # C_(i,j) = group i predicted to be in group j\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    percision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Percision: {percision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    # print(f\"Confusion Matrix: \\n {cm}\")\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.title('Confusion Matrix')\n",
    "    sns.heatmap(cm, annot=True, fmt='.5g')\n",
    "    return model, cm, f1, percision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(X, y, baseModel):\n",
    "    # Define the base model\n",
    "    model = baseModel(random_state=0)\n",
    "    # Data Split\n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size = 0.2, stratify=y)\n",
    "    # Fit the model\n",
    "    model.fit(xTrain, yTrain)\n",
    "    yPred = model.predict(xTest)\n",
    "    \n",
    "    # Evaluation\n",
    "    \n",
    "    return evaluate(model, yTest, yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features to keep values in the same scale and improve accuracy/stabilize  models\n",
    "def normalize(data, normalizerObject):\n",
    "    \"\"\"Normalize the data depending on the choice of normalizerObjecvt.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): pandas DataFrame\n",
    "        normalizerObject (sklearn type of normalizer): ex) StandardScaler\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: normalized data\n",
    "        Normalizer Object: fitted normalizer\n",
    "    \"\"\"\n",
    "    sc = normalizerObject()\n",
    "    xTrainNumericalNormalized = pd.DataFrame(sc.fit_transform(data), columns = data.columns)\n",
    "    \n",
    "    return xTrainNumericalNormalized, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalFeatures = xUnder.dtypes[xUnder.dtypes == 'float64'].index\n",
    "binaryFeatures = xUnder.dtypes[xUnder.dtypes != 'float64'].index\n",
    "\n",
    "xNumericalNormalized, sc = normalize(xUnder[numericalFeatures], StandardScaler)\n",
    "\n",
    "xNormalized = pd.concat([xNumericalNormalized, xUnder[binaryFeatures]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xFinal, yFinal = xNormalized, yUnder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR, cm, f1, percision, recall = modeling(xFinal, yFinal, LogisticRegression)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "modelXGB, cm, f1, percision, recall = modeling(xFinal, yFinal, xgb.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "plot_importance(modelXGB, importance_type=\"gain\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scotiabank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9bc83a6c1ff07071eeba09d414ddd4a4d18bd9f8f48b25cb05048b92844e59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
