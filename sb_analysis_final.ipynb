{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "trainFileName = 'train_ScotiaDSD.csv'\n",
    "testFileName = 'test_ScotiaDSD.csv'\n",
    "data = pd.read_csv(os.path.join(os.getcwd(), trainFileName))\n",
    "evalData = pd.read_csv(os.path.join(os.getcwd(), testFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARY_COLUMNS (not including FRAUD_FLAG), STRING_COLUMNS, NUMERICAL_COLUMNS\n",
    "BINARY_COLUMNS = [\n",
    " 'CARD_NOT_PRESENT',\n",
    " 'FLAG_LX',\n",
    " 'FLAG_ATM',\n",
    " 'FLAG_AUTO',\n",
    " 'FLAG_CASH',\n",
    " 'FLAG_LS',\n",
    " 'FLAG_DISCOUNT',\n",
    " 'FLAG_RECREA',\n",
    " 'FLAG_ELCTRNCS',\n",
    " 'FLAG_REG_AMT',\n",
    " 'FLAG_FASTFOOD',\n",
    " 'FLAG_GAS',\n",
    " 'FLAG_HIGH_AMT',\n",
    " 'FLAG_HIGH_RECREA',\n",
    " 'FLAG_INTERNET',\n",
    " 'FLAG_INTERNATIONAL',\n",
    " 'FLAG_JEWELRY',\n",
    " 'FLAG_LOW_AMT',\n",
    " 'FLAG_MANUAL_ENTRY',\n",
    " 'FLAG_PHONE_ORDER',\n",
    " 'FLAG_PURCHASE_EXCLUDING_GAS',\n",
    " 'FLAG_PLANNED',\n",
    " 'FLAG_RISKY',\n",
    " 'FLAG_SWIPE',\n",
    " 'FLAG_TRAVEL_ONLY',\n",
    " 'FLAG_TRAVEL_AND_ENTERTAINMENT',\n",
    " 'FLAG_WEEKEND']\n",
    "\n",
    "STRING_COLUMNS = ['TRANSACTION_ID',\n",
    "'USER_AGENT',\n",
    "'CITY',\n",
    "'EVENT_TIME']\n",
    "\n",
    "NUMERICAL_COLUMNS = ['EVENT_MONTH',\n",
    " 'EVENT_DAY_OF_WEEK',\n",
    " 'AVAIL_CRDT',\n",
    " 'AMOUNT',\n",
    " 'CREDIT_LIMIT',\n",
    " 'MEAN_AUTO_PAST_7DAY',\n",
    " 'MEAN_LS_PAST_7DAY',\n",
    " 'MEAN_RECREA_PAST_7DAY',\n",
    " 'MEAN_REG_AMT_PAST_7DAY',\n",
    " 'MEAN_FASTFOOD_PAST_7DAY',\n",
    " 'MEAN_HIGH_AMT_PAST_7DAY',\n",
    " 'MEAN_HIGH_RECREA_PAST_7DAY',\n",
    " 'MEAN_INTERNET_PAST_7DAY',\n",
    " 'MEAN_INTERNATIONAL_PAST_7DAY',\n",
    " 'MEAN_JEWELRY_PAST_7DAY',\n",
    " 'MEAN_LOW_AMT_PAST_7DAY',\n",
    " 'MEAN_MANUAL_ENTRY_PAST_7DAY',\n",
    " 'MEAN_PHONE_ORDER_PAST_7DAY',\n",
    " 'MEAN_PLANNED_PAST_7DAY',\n",
    " 'MEAN_SWIPE_PAST_7DAY',\n",
    " 'MEAN_TRAVEL_AND_ENTERTAINMENT_PAST_7DAY',\n",
    " 'MEAN_WEEKEND_PAST_7DAY',\n",
    " 'MAX_CASH_PAST_7DAY',\n",
    " 'MAX_LS_PAST_7DAY',\n",
    " 'MAX_RECREA_PAST_7DAY',\n",
    " 'MAX_HIGH_AMT_PAST_7DAY',\n",
    " 'MAX_HIGH_RECREA_PAST_7DAY',\n",
    " 'MAX_INTERNET_PAST_7DAY',\n",
    " 'MAX_PHONE_ORDER_PAST_7DAY',\n",
    " 'MAX_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'MAX_SWIPE_PAST_7DAY',\n",
    " 'MAX_WEEKEND_PAST_7DAY',\n",
    " 'STD_LX_PAST_7DAY',\n",
    " 'STD_FASTFOOD_PAST_7DAY',\n",
    " 'STD_HIGH_AMT_PAST_7DAY',\n",
    " 'STD_INTERNET_PAST_7DAY',\n",
    " 'STD_LOW_AMT_PAST_7DAY',\n",
    " 'STD_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'STD_SWIPE_PAST_7DAY',\n",
    " 'STD_TRAVEL_AND_ENTERTAINMENT_PAST_7DAY',\n",
    " 'SUM_LX_PAST_7DAY',\n",
    " 'SUM_AUTO_PAST_7DAY',\n",
    " 'SUM_LS_PAST_7DAY',\n",
    " 'SUM_RECREA_PAST_7DAY',\n",
    " 'SUM_GAS_PAST_7DAY',\n",
    " 'SUM_HIGH_AMT_PAST_7DAY',\n",
    " 'SUM_INTERNET_PAST_7DAY',\n",
    " 'SUM_INTERNATIONAL_PAST_7DAY',\n",
    " 'SUM_LOW_AMT_PAST_7DAY',\n",
    " 'SUM_MANUAL_ENTRY_PAST_7DAY',\n",
    " 'SUM_PHONE_ORDER_PAST_7DAY',\n",
    " 'SUM_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'SUM_PARTIAL_PAST_7DAY',\n",
    " 'SUM_PLANNED_PAST_7DAY',\n",
    " 'SUM_SWIPE_PAST_7DAY',\n",
    " 'SUM_WEEKEND_PAST_7DAY',\n",
    " 'COUNT_AUTO_PAST_7DAY',\n",
    " 'COUNT_ELCTRNCS_PAST_7DAY',\n",
    " 'COUNT_GAS_PAST_7DAY',\n",
    " 'COUNT_HIGH_AMT_PAST_7DAY',\n",
    " 'COUNT_INTERNET_PAST_7DAY',\n",
    " 'COUNT_LOW_AMT_PAST_7DAY',\n",
    " 'COUNT_MANUAL_ENTRY_PAST_7DAY',\n",
    " 'COUNT_PHONE_ORDER_PAST_7DAY',\n",
    " 'COUNT_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'COUNT_SWIPE_PAST_7DAY',\n",
    " 'COUNT_TRAVEL_AND_ENTERTAINMENT_PAST_7DAY',\n",
    " 'COUNT_WEEKEND_PAST_7DAY',\n",
    " 'MEAN_AUTO_PAST_30DAY',\n",
    " 'MEAN_DISCOUNT_PAST_30DAY',\n",
    " 'MEAN_RECREA_PAST_30DAY',\n",
    " 'MEAN_ELCTRNCS_PAST_30DAY',\n",
    " 'MEAN_REG_AMT_PAST_30DAY',\n",
    " 'MEAN_HIGH_AMT_PAST_30DAY',\n",
    " 'MEAN_INTERNET_PAST_30DAY',\n",
    " 'MEAN_LOW_AMT_PAST_30DAY',\n",
    " 'MEAN_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'MEAN_PHONE_ORDER_PAST_30DAY',\n",
    " 'MEAN_PURCHASE_EXCLUDING_GAS_PAST_30DAY',\n",
    " 'MEAN_PLANNED_PAST_30DAY',\n",
    " 'MEAN_SWIPE_PAST_30DAY',\n",
    " 'MEAN_TRAVEL_AND_ENTERTAINMENT_PAST_30DAY',\n",
    " 'MEAN_WEEKEND_PAST_30DAY',\n",
    " 'MAX_AUTO_PAST_30DAY',\n",
    " 'MAX_LS_PAST_30DAY',\n",
    " 'MAX_ELCTRNCS_PAST_30DAY',\n",
    " 'MAX_FASTFOOD_PAST_30DAY',\n",
    " 'MAX_HIGH_RECREA_PAST_30DAY',\n",
    " 'MAX_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'MAX_PHONE_ORDER_PAST_30DAY',\n",
    " 'MAX_PARTIAL_PAST_30DAY',\n",
    " 'MAX_RISKY_PAST_30DAY',\n",
    " 'MAX_WEEKEND_PAST_30DAY',\n",
    " 'STD_AUTO_PAST_30DAY',\n",
    " 'STD_LS_PAST_30DAY',\n",
    " 'STD_RECREA_PAST_30DAY',\n",
    " 'STD_ELCTRNCS_PAST_30DAY',\n",
    " 'STD_REG_AMT_PAST_30DAY',\n",
    " 'STD_HIGH_RECREA_PAST_30DAY',\n",
    " 'STD_INTERNET_PAST_30DAY',\n",
    " 'STD_LOW_AMT_PAST_30DAY',\n",
    " 'STD_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'STD_PHONE_ORDER_PAST_30DAY',\n",
    " 'STD_PARTIAL_PAST_30DAY',\n",
    " 'STD_SWIPE_PAST_30DAY',\n",
    " 'STD_TRAVEL_ONLY_PAST_30DAY',\n",
    " 'STD_TRAVEL_AND_ENTERTAINMENT_PAST_30DAY',\n",
    " 'SUM_AUTO_PAST_30DAY',\n",
    " 'SUM_LS_PAST_30DAY',\n",
    " 'SUM_DISCOUNT_PAST_30DAY',\n",
    " 'SUM_RECREA_PAST_30DAY',\n",
    " 'SUM_ELCTRNCS_PAST_30DAY',\n",
    " 'SUM_REG_AMT_PAST_30DAY',\n",
    " 'SUM_FASTFOOD_PAST_30DAY',\n",
    " 'SUM_GAS_PAST_30DAY',\n",
    " 'SUM_HIGH_AMT_PAST_30DAY',\n",
    " 'SUM_HIGH_RECREA_PAST_30DAY',\n",
    " 'SUM_INTERNET_PAST_30DAY',\n",
    " 'SUM_INTERNATIONAL_PAST_30DAY',\n",
    " 'SUM_LOW_AMT_PAST_30DAY',\n",
    " 'SUM_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'SUM_PHONE_ORDER_PAST_30DAY',\n",
    " 'SUM_PURCHASE_EXCLUDING_GAS_PAST_30DAY',\n",
    " 'SUM_SWIPE_PAST_30DAY',\n",
    " 'SUM_TRAVEL_ONLY_PAST_30DAY',\n",
    " 'SUM_WEEKEND_PAST_30DAY',\n",
    " 'COUNT_AUTO_PAST_30DAY',\n",
    " 'COUNT_RECREA_PAST_30DAY',\n",
    " 'COUNT_REG_AMT_PAST_30DAY',\n",
    " 'COUNT_FASTFOOD_PAST_30DAY',\n",
    " 'COUNT_GAS_PAST_30DAY',\n",
    " 'COUNT_HIGH_AMT_PAST_30DAY',\n",
    " 'COUNT_INTERNET_PAST_30DAY',\n",
    " 'COUNT_LOW_AMT_PAST_30DAY',\n",
    " 'COUNT_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'COUNT_PHONE_ORDER_PAST_30DAY',\n",
    " 'COUNT_PURCHASE_EXCLUDING_GAS_PAST_30DAY',\n",
    " 'COUNT_PLANNED_PAST_30DAY',\n",
    " 'COUNT_SWIPE_PAST_30DAY',\n",
    " 'COUNT_TRAVEL_AND_ENTERTAINMENT_PAST_30DAY',\n",
    " 'COUNT_WEEKEND_PAST_30DAY',\n",
    " 'PREV_M_INFLATION',\n",
    " 'PREV_M_UNEMP_RATE']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "Determine any distinct qualities of the data and decide if any changes are required.\n",
    "\n",
    "### Results\n",
    "After having a deep-dive into the dataset, the following conclusions can be made:\n",
    "- there are three distinct data types:\n",
    "    - float64 (numerical)\n",
    "    - int64 (binary)\n",
    "    - object (string)\n",
    "The string columns need to be converted into useful numerical features or dropped. For convention purposes, convert all numerical columns into floats and all binary into int (some counts are ints in the data). The actual string columns may be too granular for the model hence were removed.\n",
    "- the magnitude of the variables differ greatly so normalized is required.\n",
    "- imbalance of target data (97.5% to 2.5%), therefore oversampling methods need to be used.\n",
    "- The columns with NAs are unnecessary:\n",
    "    - for all cases where CITY and USER_AGENT is NA, it is when FLAG_INTERNET == 0\n",
    "    - keep only the FLAG_INTERNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Description of Data\n",
    "display(data.info())\n",
    "display(data.head())\n",
    "display(data.describe())\n",
    "print(f'Number of Duplicated Rows: {data.duplicated().sum()}')\n",
    "print(f'Number of Duplicated Columns: {len(data.columns) == len(np.unique(data.columns))}')\n",
    "print(f\"Proportion of Fraud vs Non-Fraud Transactions: \\n {data['FRAUD_FLAG'].value_counts(normalize=True)}\")\n",
    "dataMissing = data.isna().sum(axis=0)\n",
    "print(f\"Number of Rows with NA: \\n {dataMissing[dataMissing != 0]}\")\n",
    "print(f\"Disparity between magnitude of Numerical Values: {data[NUMERICAL_COLUMNS].to_numpy().max()} and {data[NUMERICAL_COLUMNS].to_numpy().min()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "Apply the findings from the Data Analysis step to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataframe):\n",
    "    \"\"\" Pre-process the data.\n",
    "            - convert dtypes\n",
    "            - change units to something more useful\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: same pandas Dataframe but preprocessed\n",
    "    \"\"\"\n",
    "    # Convert binary variables into integers\n",
    "    \n",
    "    nonBinaryColumnNamesList = []\n",
    "    for columnName in dataframe:\n",
    "        # Binary variables only have 2 unique values\n",
    "        if len(dataframe[columnName].unique()) == 2:\n",
    "            dataframe[columnName] = dataframe[columnName].astype('int64')\n",
    "        elif (dataframe[columnName].dtype == 'int64') or (dataframe[columnName].dtype == 'float64'):\n",
    "            dataframe[columnName] = dataframe[columnName].astype('float64')\n",
    "\n",
    "    # Convert EVENT_TIME string into seconds to be useful\n",
    "    dataframe['EVENT_TIME'] = dataframe['EVENT_TIME'].apply(lambda x: x[:-1] if x[-1] == ':' else x)\n",
    "    dateTransactionTime = pd.DataFrame(dataframe['EVENT_TIME'].str.split(r':').to_list(), columns=['hour', 'minutes'])\n",
    "    dateTransactionTime = dateTransactionTime.astype('float64')\n",
    "\n",
    "    dataTransactionTimeSeconds = dateTransactionTime['hour'] * 60 * 60 + dateTransactionTime['minutes'] * 60\n",
    "    \n",
    "    # Drop the useless columns now\n",
    "    dataframe = dataframe.drop(STRING_COLUMNS, axis = 1)\n",
    "    dataframe['EVENT_TIME_IN_SECONDS'] = dataTransactionTimeSeconds\n",
    "    NUMERICAL_COLUMNS.append('EVENT_TIME_IN_SECONDS')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPreprocessed = data_preprocessing(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countNumberOfIQRTestOutliers(dataframe, threshold):\n",
    "\n",
    "    # IQR Test    \n",
    "    mean = np.mean(dataframe, axis=0)\n",
    "    std = np.std(dataframe, axis=0)\n",
    "\n",
    "    min = mean - threshold * std\n",
    "    max = mean + threshold * std\n",
    "    \n",
    "    # If a row has at least 1 column that fails IQR test, true\n",
    "    outlierMask = (dataframe < min) | (dataframe > max)\n",
    "    rowOutlierMask = np.any(outlierMask, axis=1)\n",
    "    rowOutlierCount = np.sum(outlierMask, axis=1)\n",
    "\n",
    "    return rowOutlierCount\n",
    "\n",
    "\n",
    "def majorityVoteOutlierAlgorithm(dataframe, thresholdIQR, thresholdProportion):\n",
    "\n",
    "    # IQR test to remove outliers        \n",
    "    rowOutlierCount = countNumberOfIQRTestOutliers(dataframe, thresholdIQR)\n",
    "    featureSize = len(dataframe.columns)\n",
    "\n",
    "    # If more than thresholdProportion of the features are outliers according to IQR Test, remove them\n",
    "    majorityOutlierMask = rowOutlierCount >= (featureSize * thresholdProportion)\n",
    "\n",
    "    return dataframe.loc[~majorityOutlierMask, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataProcessed = majorityVoteOutlierAlgorithm(dataPreprocessed, thresholdIQR=2.5, thresholdProportion=0.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataPreprocessed['FRAUD_FLAG']\n",
    "X = dataPreprocessed.drop(['FRAUD_FLAG'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlationMatrixPredictorVsTarget(X, y, absoluteThreshold):\n",
    "    corr_list = []\n",
    "    for col in X:\n",
    "        corr_list.append(np.corrcoef(X[col], y)[0,1])\n",
    "    corr_list.sort()\n",
    "    \n",
    "    fig,ax=plt.subplots(figsize=(32,20))\n",
    "    for label in (ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        label.set_fontsize(10)\n",
    "\n",
    "    ax.barh(X.columns, corr_list)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    corrArray = np.array(corr_list)\n",
    "    corrMask = np.abs(corrArray) >= absoluteThreshold\n",
    "\n",
    "    sufficientCorrColumns = np.array(X.columns)[np.where(corrMask)[0]]\n",
    "    \n",
    "    return X[sufficientCorrColumns], sufficientCorrColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelection(X, y, featureImportanceThreshold, method):\n",
    "\n",
    "    if method == 'rf':\n",
    "        rf = RandomForestClassifier()\n",
    "        rf.fit(X, y)\n",
    "\n",
    "        # Greatest to least\n",
    "        sort = (-rf.feature_importances_).argsort()\n",
    "        featureImportanceColumns = X.columns[sort]\n",
    "        featureImportanceValues = rf.feature_importances_[sort]\n",
    "\n",
    "        # Plot the importance (sorted)\n",
    "        print(featureImportanceColumns)\n",
    "        print(np.where(featureImportanceColumns == 'FLAG_INTERNET'))\n",
    "        plt.barh(featureImportanceColumns, featureImportanceValues)\n",
    "        plt.xlabel(\"Feature Importance\")\n",
    "        \n",
    "        # Keep only interesying columns\n",
    "        keepColumns = featureImportanceColumns[featureImportanceValues >= featureImportanceThreshold]\n",
    "\n",
    "        print(f'Original Column Count: {X.shape[1]}')\n",
    "        print(f'New Column Count: {len(keepColumns)}')\n",
    "        \n",
    "    return keepColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featureImportanceThreshold = 0.002\n",
    "\n",
    "# # For DEBUG purposes:\n",
    "# xRF, _, yRF, _ = train_test_split(X, y, train_size=0.1, stratify=y, random_state=0)\n",
    "# # -------------------\n",
    "# importFeatures = featureSelection(xRF, yRF, featureImportanceThreshold, method='rf')\n",
    "# xImportant = X[importFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# n_components=50\n",
    "# pca = PCA(n_components)\n",
    "# xPCA = pca.fit_transform(X)\n",
    "\n",
    "# xImportant = pd.DataFrame(xPCA, columns = [f'V{i}'for i in range(n_components)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xImportant, sufficientCorrColumns = correlationMatrixPredictorVsTarget(X, y, absoluteThreshold=0.035)\n",
    "len(sufficientCorrColumns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "\n",
    "### Synthetic Minority Oversampling TEchniques (SMOTE)\n",
    "\n",
    "#### Description:\n",
    "- randomly select a point in the minority class\n",
    "- calculate the euclidean distance between each point in the minority class and find the $k$ nearest points\n",
    "- randomly select a point among the $k$ points\n",
    "- generate new data point (randomly on the line, the weight of the equation between two points is randomly generated as well)\n",
    "\n",
    "### SMOTE-NC (Nominal Continuous)\n",
    "Extension of SMOTE. In order to include categorical features into the synthetic data generation:\n",
    "- randomly select a point in the minority class\n",
    "- calculate the euclidean distance between each point in the minority class and find the $k$ nearest points\n",
    "    - for each nominal category, substitute in the euclidean distance calculation the MEDIAN of STD of the continous classes\n",
    "- randomly select a point among the $k$ points\n",
    "- generate new data point\n",
    "    - for continous values, same as SMOTE\n",
    "    - for nominal values, take the randomly selected points nominal value\n",
    "\n",
    "### SMOTE-ENC (Encoded Nominal and Continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSampling(X, y, testSize, methods):\n",
    "    xNew, yNew = X, y\n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        print(f\"Original: \\n{yNew.value_counts()}\")\n",
    "        if (method == 'SMOTE') and np.any(np.where(xNew.dtypes == 'int64')[0]):\n",
    "            sampler = SMOTENC(categorical_features=np.where(xNew.dtypes == 'int64')[0], sampling_strategy=0.5, random_state=0)\n",
    "        elif method == 'SMOTENC':\n",
    "            sampler = SMOTENC(categorical_features=np.where(xNew.dtypes == 'int64')[0], sampling_strategy=0.5, random_state=0)\n",
    "        elif method == 'OVER_SAMPLE':\n",
    "            sampler = RandomOverSampler(sampling_strategy=0.5, random_state=0)\n",
    "        elif method == 'UNDER_SAMPLE':\n",
    "            sampler = RandomUnderSampler(sampling_strategy=0.8)\n",
    "        \n",
    "        xNew, yNew = sampler.fit_resample(xNew, yNew)\n",
    "        print(f\"New: \\n{yNew.value_counts()}\")\n",
    "        print('---')\n",
    "    return xNew, yNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainDS, xTestDS, yTrainDS, yTestDS = train_test_split(xImportant, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# print(xTrainDS.shape, xTestDS.shape, \"\\n\" , yTrainDS.value_counts(), \"\\n\", yTestDS.value_counts())\n",
    "\n",
    "methods = ['SMOTENC', 'UNDER_SAMPLE']\n",
    "xTrainSampled, yTrainSampled = dataSampling(xTrainDS, yTrainDS, testSize=0.2, methods=methods)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, y_true, y_pred):\n",
    "    # C_(i,j) = group i predicted to be in group j\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    percision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Percision: {percision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    # print(f\"Confusion Matrix: \\n {cm}\")\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.title('Confusion Matrix')\n",
    "    sns.heatmap(cm, annot=True, fmt='.5g')\n",
    "    return model, cm, f1, percision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(xTrain, xTest, yTrain, yTest, baseModel):\n",
    "    # Define the base model\n",
    "    model = baseModel(random_state=0)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(xTrain, yTrain)\n",
    "    yPred = model.predict(xTest)\n",
    "    \n",
    "    # Evaluation\n",
    "    model, cm, f1, percision, recall = evaluate(model, yTest, yPred)\n",
    "    return model, cm, f1, percision, recall, xTrain, xTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features to keep values in the same scale and improve accuracy/stabilize  models\n",
    "def normalize(data, normalizerObject, train=True):\n",
    "    \"\"\"Normalize the data depending on the choice of normalizerObjecvt.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): pandas DataFrame\n",
    "        normalizerObject (sklearn type of normalizer): ex) StandardScaler\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: normalized data\n",
    "        Normalizer Object: fitted normalizer\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        sc = normalizerObject()\n",
    "        xNumericalNormalized = pd.DataFrame(sc.fit_transform(data), columns = data.columns, index=data.index)\n",
    "    else:\n",
    "        sc = normalizerObject\n",
    "        xNumericalNormalized = pd.DataFrame(sc.transform(data), columns = data.columns, index=data.index)\n",
    "        \n",
    "    return xNumericalNormalized, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericalFeatures = xTrainSampled.dtypes[xTrainSampled.dtypes == 'float64'].index\n",
    "binaryFeatures = xTrainSampled.dtypes[xTrainSampled.dtypes != 'float64'].index\n",
    "\n",
    "xNumericalNormalized, sc = normalize(xTrainSampled[numericalFeatures], StandardScaler, train=True)\n",
    "xTestNumericalNormalized, _ = normalize(xTestDS[numericalFeatures], sc, train=False)\n",
    "\n",
    "xNormalized = pd.concat([xNumericalNormalized, xTrainSampled[binaryFeatures]], axis=1)\n",
    "\n",
    "xTestNormalized = pd.concat([xTestNumericalNormalized, xTestDS[binaryFeatures]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=30)\n",
    "# xPCA = pca.fit_transform(xNormalized)\n",
    "# xTestPCA = pca.transform(xTestNormalized)\n",
    "\n",
    "# pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xFinal, yFinal = xNormalized, yTrainSampled\n",
    "xTestFinal, yTestFinal = xTestNormalized, yTestDS\n",
    "\n",
    "print(xFinal.shape, xTestFinal.shape, yFinal.shape, yTestFinal.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logisitc Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR, cm, f1, percision, recall, xTrain, xTest, yTrain, yTest = modeling(xFinal, xTestFinal, yFinal, yTestFinal, LogisticRegression)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "modelXGB, cm, f1, percision, recall, xTrain, xTest, yTrain, yTest = modeling(xFinal, xTestFinal, yFinal, yTestFinal, xgb.XGBClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = modelXGB.predict(xTestFinal)\n",
    "\n",
    "fpMask = (yPred == 1) == (yTestFinal == 0)\n",
    "fnMask = (yPred == 0) == (yTestFinal == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpXTest = xTestFinal[fpMask]\n",
    "fnXTest = xTestFinal[fnMask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpXTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "\n",
    "plot_importance(modelXGB, importance_type=\"cover\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# modelXGB, cm, f1, percision, recall, xTrain, xTest, yTrain, yTest = modeling(xFinal, xTestFinal, yFinal, yTestFinal, IsolationForest)\n",
    "\n",
    "\n",
    "isoF = IsolationForest()\n",
    "isoF.fit(xFinal)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yPred = modelLR.predict(xTest)\n",
    "\n",
    "# yFN = np.where((yPred == 0) & (yTest == 1))\n",
    "# yFP = np.where((yPred == 1) & (yTest == 0))\n",
    "\n",
    "# xTestFN = xTest.iloc[yFN[0], :]\n",
    "# xTestFP = xTest.iloc[yFP[0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTestDSNormalized = pd.DataFrame(sc.transform(xTestDS[numericalFeatures]), columns=numericalFeatures)\n",
    "xTestDSNormalized = pd.concat([xTestDSNormalized, xTestDS[binaryFeatures].reset_index()], axis=1)\n",
    "\n",
    "xTestFinal, yTestFinal = xTestDSNormalized, yTestDS\n",
    "xTestFinal = xTestFinal.drop([\"index\"], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scotiabank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9bc83a6c1ff07071eeba09d414ddd4a4d18bd9f8f48b25cb05048b92844e59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
