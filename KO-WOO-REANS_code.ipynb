{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# Sampling\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, SMOTENC\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "trainFileName = 'train_ScotiaDSD.csv'\n",
    "testFileName = 'test_ScotiaDSD.csv'\n",
    "data = pd.read_csv(os.path.join(os.getcwd(), trainFileName))\n",
    "evalData = pd.read_csv(os.path.join(os.getcwd(), testFileName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BINARY_COLUMNS (not including FRAUD_FLAG), STRING_COLUMNS, NUMERICAL_COLUMNS\n",
    "BINARY_COLUMNS = [\n",
    " 'CARD_NOT_PRESENT',\n",
    " 'FLAG_LX',\n",
    " 'FLAG_ATM',\n",
    " 'FLAG_AUTO',\n",
    " 'FLAG_CASH',\n",
    " 'FLAG_LS',\n",
    " 'FLAG_DISCOUNT',\n",
    " 'FLAG_RECREA',\n",
    " 'FLAG_ELCTRNCS',\n",
    " 'FLAG_REG_AMT',\n",
    " 'FLAG_FASTFOOD',\n",
    " 'FLAG_GAS',\n",
    " 'FLAG_HIGH_AMT',\n",
    " 'FLAG_HIGH_RECREA',\n",
    " 'FLAG_INTERNET',\n",
    " 'FLAG_INTERNATIONAL',\n",
    " 'FLAG_JEWELRY',\n",
    " 'FLAG_LOW_AMT',\n",
    " 'FLAG_MANUAL_ENTRY',\n",
    " 'FLAG_PHONE_ORDER',\n",
    " 'FLAG_PURCHASE_EXCLUDING_GAS',\n",
    " 'FLAG_PLANNED',\n",
    " 'FLAG_RISKY',\n",
    " 'FLAG_SWIPE',\n",
    " 'FLAG_TRAVEL_ONLY',\n",
    " 'FLAG_TRAVEL_AND_ENTERTAINMENT',\n",
    " 'FLAG_WEEKEND']\n",
    "\n",
    "STRING_COLUMNS = ['TRANSACTION_ID',\n",
    "'USER_AGENT',\n",
    "'CITY',\n",
    "'EVENT_TIME']\n",
    "\n",
    "NUMERICAL_COLUMNS = ['EVENT_MONTH',\n",
    " 'EVENT_DAY_OF_WEEK',\n",
    " 'AVAIL_CRDT',\n",
    " 'AMOUNT',\n",
    " 'CREDIT_LIMIT',\n",
    " 'MEAN_AUTO_PAST_7DAY',\n",
    " 'MEAN_LS_PAST_7DAY',\n",
    " 'MEAN_RECREA_PAST_7DAY',\n",
    " 'MEAN_REG_AMT_PAST_7DAY',\n",
    " 'MEAN_FASTFOOD_PAST_7DAY',\n",
    " 'MEAN_HIGH_AMT_PAST_7DAY',\n",
    " 'MEAN_HIGH_RECREA_PAST_7DAY',\n",
    " 'MEAN_INTERNET_PAST_7DAY',\n",
    " 'MEAN_INTERNATIONAL_PAST_7DAY',\n",
    " 'MEAN_JEWELRY_PAST_7DAY',\n",
    " 'MEAN_LOW_AMT_PAST_7DAY',\n",
    " 'MEAN_MANUAL_ENTRY_PAST_7DAY',\n",
    " 'MEAN_PHONE_ORDER_PAST_7DAY',\n",
    " 'MEAN_PLANNED_PAST_7DAY',\n",
    " 'MEAN_SWIPE_PAST_7DAY',\n",
    " 'MEAN_TRAVEL_AND_ENTERTAINMENT_PAST_7DAY',\n",
    " 'MEAN_WEEKEND_PAST_7DAY',\n",
    " 'MAX_CASH_PAST_7DAY',\n",
    " 'MAX_LS_PAST_7DAY',\n",
    " 'MAX_RECREA_PAST_7DAY',\n",
    " 'MAX_HIGH_AMT_PAST_7DAY',\n",
    " 'MAX_HIGH_RECREA_PAST_7DAY',\n",
    " 'MAX_INTERNET_PAST_7DAY',\n",
    " 'MAX_PHONE_ORDER_PAST_7DAY',\n",
    " 'MAX_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'MAX_SWIPE_PAST_7DAY',\n",
    " 'MAX_WEEKEND_PAST_7DAY',\n",
    " 'STD_LX_PAST_7DAY',\n",
    " 'STD_FASTFOOD_PAST_7DAY',\n",
    " 'STD_HIGH_AMT_PAST_7DAY',\n",
    " 'STD_INTERNET_PAST_7DAY',\n",
    " 'STD_LOW_AMT_PAST_7DAY',\n",
    " 'STD_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'STD_SWIPE_PAST_7DAY',\n",
    " 'STD_TRAVEL_AND_ENTERTAINMENT_PAST_7DAY',\n",
    " 'SUM_LX_PAST_7DAY',\n",
    " 'SUM_AUTO_PAST_7DAY',\n",
    " 'SUM_LS_PAST_7DAY',\n",
    " 'SUM_RECREA_PAST_7DAY',\n",
    " 'SUM_GAS_PAST_7DAY',\n",
    " 'SUM_HIGH_AMT_PAST_7DAY',\n",
    " 'SUM_INTERNET_PAST_7DAY',\n",
    " 'SUM_INTERNATIONAL_PAST_7DAY',\n",
    " 'SUM_LOW_AMT_PAST_7DAY',\n",
    " 'SUM_MANUAL_ENTRY_PAST_7DAY',\n",
    " 'SUM_PHONE_ORDER_PAST_7DAY',\n",
    " 'SUM_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'SUM_PARTIAL_PAST_7DAY',\n",
    " 'SUM_PLANNED_PAST_7DAY',\n",
    " 'SUM_SWIPE_PAST_7DAY',\n",
    " 'SUM_WEEKEND_PAST_7DAY',\n",
    " 'COUNT_AUTO_PAST_7DAY',\n",
    " 'COUNT_ELCTRNCS_PAST_7DAY',\n",
    " 'COUNT_GAS_PAST_7DAY',\n",
    " 'COUNT_HIGH_AMT_PAST_7DAY',\n",
    " 'COUNT_INTERNET_PAST_7DAY',\n",
    " 'COUNT_LOW_AMT_PAST_7DAY',\n",
    " 'COUNT_MANUAL_ENTRY_PAST_7DAY',\n",
    " 'COUNT_PHONE_ORDER_PAST_7DAY',\n",
    " 'COUNT_PURCHASE_EXCLUDING_GAS_PAST_7DAY',\n",
    " 'COUNT_SWIPE_PAST_7DAY',\n",
    " 'COUNT_TRAVEL_AND_ENTERTAINMENT_PAST_7DAY',\n",
    " 'COUNT_WEEKEND_PAST_7DAY',\n",
    " 'MEAN_AUTO_PAST_30DAY',\n",
    " 'MEAN_DISCOUNT_PAST_30DAY',\n",
    " 'MEAN_RECREA_PAST_30DAY',\n",
    " 'MEAN_ELCTRNCS_PAST_30DAY',\n",
    " 'MEAN_REG_AMT_PAST_30DAY',\n",
    " 'MEAN_HIGH_AMT_PAST_30DAY',\n",
    " 'MEAN_INTERNET_PAST_30DAY',\n",
    " 'MEAN_LOW_AMT_PAST_30DAY',\n",
    " 'MEAN_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'MEAN_PHONE_ORDER_PAST_30DAY',\n",
    " 'MEAN_PURCHASE_EXCLUDING_GAS_PAST_30DAY',\n",
    " 'MEAN_PLANNED_PAST_30DAY',\n",
    " 'MEAN_SWIPE_PAST_30DAY',\n",
    " 'MEAN_TRAVEL_AND_ENTERTAINMENT_PAST_30DAY',\n",
    " 'MEAN_WEEKEND_PAST_30DAY',\n",
    " 'MAX_AUTO_PAST_30DAY',\n",
    " 'MAX_LS_PAST_30DAY',\n",
    " 'MAX_ELCTRNCS_PAST_30DAY',\n",
    " 'MAX_FASTFOOD_PAST_30DAY',\n",
    " 'MAX_HIGH_RECREA_PAST_30DAY',\n",
    " 'MAX_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'MAX_PHONE_ORDER_PAST_30DAY',\n",
    " 'MAX_PARTIAL_PAST_30DAY',\n",
    " 'MAX_RISKY_PAST_30DAY',\n",
    " 'MAX_WEEKEND_PAST_30DAY',\n",
    " 'STD_AUTO_PAST_30DAY',\n",
    " 'STD_LS_PAST_30DAY',\n",
    " 'STD_RECREA_PAST_30DAY',\n",
    " 'STD_ELCTRNCS_PAST_30DAY',\n",
    " 'STD_REG_AMT_PAST_30DAY',\n",
    " 'STD_HIGH_RECREA_PAST_30DAY',\n",
    " 'STD_INTERNET_PAST_30DAY',\n",
    " 'STD_LOW_AMT_PAST_30DAY',\n",
    " 'STD_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'STD_PHONE_ORDER_PAST_30DAY',\n",
    " 'STD_PARTIAL_PAST_30DAY',\n",
    " 'STD_SWIPE_PAST_30DAY',\n",
    " 'STD_TRAVEL_ONLY_PAST_30DAY',\n",
    " 'STD_TRAVEL_AND_ENTERTAINMENT_PAST_30DAY',\n",
    " 'SUM_AUTO_PAST_30DAY',\n",
    " 'SUM_LS_PAST_30DAY',\n",
    " 'SUM_DISCOUNT_PAST_30DAY',\n",
    " 'SUM_RECREA_PAST_30DAY',\n",
    " 'SUM_ELCTRNCS_PAST_30DAY',\n",
    " 'SUM_REG_AMT_PAST_30DAY',\n",
    " 'SUM_FASTFOOD_PAST_30DAY',\n",
    " 'SUM_GAS_PAST_30DAY',\n",
    " 'SUM_HIGH_AMT_PAST_30DAY',\n",
    " 'SUM_HIGH_RECREA_PAST_30DAY',\n",
    " 'SUM_INTERNET_PAST_30DAY',\n",
    " 'SUM_INTERNATIONAL_PAST_30DAY',\n",
    " 'SUM_LOW_AMT_PAST_30DAY',\n",
    " 'SUM_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'SUM_PHONE_ORDER_PAST_30DAY',\n",
    " 'SUM_PURCHASE_EXCLUDING_GAS_PAST_30DAY',\n",
    " 'SUM_SWIPE_PAST_30DAY',\n",
    " 'SUM_TRAVEL_ONLY_PAST_30DAY',\n",
    " 'SUM_WEEKEND_PAST_30DAY',\n",
    " 'COUNT_AUTO_PAST_30DAY',\n",
    " 'COUNT_RECREA_PAST_30DAY',\n",
    " 'COUNT_REG_AMT_PAST_30DAY',\n",
    " 'COUNT_FASTFOOD_PAST_30DAY',\n",
    " 'COUNT_GAS_PAST_30DAY',\n",
    " 'COUNT_HIGH_AMT_PAST_30DAY',\n",
    " 'COUNT_INTERNET_PAST_30DAY',\n",
    " 'COUNT_LOW_AMT_PAST_30DAY',\n",
    " 'COUNT_MANUAL_ENTRY_PAST_30DAY',\n",
    " 'COUNT_PHONE_ORDER_PAST_30DAY',\n",
    " 'COUNT_PURCHASE_EXCLUDING_GAS_PAST_30DAY',\n",
    " 'COUNT_PLANNED_PAST_30DAY',\n",
    " 'COUNT_SWIPE_PAST_30DAY',\n",
    " 'COUNT_TRAVEL_AND_ENTERTAINMENT_PAST_30DAY',\n",
    " 'COUNT_WEEKEND_PAST_30DAY',\n",
    " 'PREV_M_INFLATION',\n",
    " 'PREV_M_UNEMP_RATE']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "Determine any distinct qualities of the data and decide if any changes are required.\n",
    "\n",
    "### Results\n",
    "After having a deep-dive into the dataset, the following conclusions can be made:\n",
    "- there are three distinct data types:\n",
    "    - float64 (numerical)\n",
    "    - int64 (binary)\n",
    "    - object (string)\n",
    "The string columns need to be converted into useful numerical features or dropped. For convention purposes, convert all numerical columns into floats and all binary into int (some counts are ints in the data). The actual string columns may be too granular for the model hence were removed.\n",
    "- the magnitude of the variables differ greatly so normalized is required.\n",
    "- imbalance of target data (97.5% to 2.5%), therefore oversampling methods need to be used.\n",
    "- The columns with NAs are unnecessary:\n",
    "    - for all cases where CITY and USER_AGENT is NA, it is when FLAG_INTERNET == 0\n",
    "    - keep only the FLAG_INTERNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Description of Data (answers above in markdown)\n",
    "display(data.info())\n",
    "display(data.head())\n",
    "display(data.describe())\n",
    "print(f'Number of Duplicated Rows: {data.duplicated().sum()}')\n",
    "print(f'Number of Duplicated Columns: {len(data.columns) == len(np.unique(data.columns))}')\n",
    "print(f\"Proportion of Fraud vs Non-Fraud Transactions: \\n {data['FRAUD_FLAG'].value_counts(normalize=True)}\")\n",
    "dataMissing = data.isna().sum(axis=0)\n",
    "print(f'Number of Rows with NA: \\n {dataMissing[dataMissing != 0]}')\n",
    "print(f'Disparity between magnitude of Numerical Values: {data[NUMERICAL_COLUMNS].to_numpy().max()} and {data[NUMERICAL_COLUMNS].to_numpy().min()}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing\n",
    "Create new features and prepare for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(dataframe):\n",
    "    \"\"\" Pre-process the data.\n",
    "            - convert dtypes\n",
    "            - change units to something more useful\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: same pandas Dataframe but preprocessed\n",
    "    \"\"\"\n",
    "    # Convert binary variables into integers\n",
    "    \n",
    "    nonBinaryColumnNamesList = []\n",
    "    for columnName in dataframe:\n",
    "        # Binary variables only have 2 unique values\n",
    "        if len(dataframe[columnName].unique()) == 2:\n",
    "            dataframe[columnName] = dataframe[columnName].astype('int64')\n",
    "        elif (dataframe[columnName].dtype == 'int64') or (dataframe[columnName].dtype == 'float64'):\n",
    "            dataframe[columnName] = dataframe[columnName].astype('float64')\n",
    "\n",
    "    # Convert EVENT_TIME string into seconds to be useful\n",
    "    dataframe['EVENT_TIME'] = dataframe['EVENT_TIME'].apply(lambda x: x[:-1] if x[-1] == ':' else x)\n",
    "    dateTransactionTime = pd.DataFrame(dataframe['EVENT_TIME'].str.split(r':').to_list(), columns=['hour', 'minutes'])\n",
    "    dateTransactionTime = dateTransactionTime.astype('float64')\n",
    "\n",
    "    dataTransactionTimeSeconds = dateTransactionTime['hour'] * 60 * 60 + dateTransactionTime['minutes'] * 60\n",
    "    \n",
    "    # Drop the useless columns now\n",
    "    dataframe = dataframe.drop(STRING_COLUMNS, axis = 1)\n",
    "    dataframe['EVENT_TIME_IN_SECONDS'] = dataTransactionTimeSeconds\n",
    "\n",
    "    # Add addtional columns\n",
    "    dataframe[\"FLAG_INTERNATIONAL_INTERNET\"] = [int(x) for x in (dataframe[\"FLAG_INTERNATIONAL\"]==1) & (dataframe[\"FLAG_INTERNET\"]==1)]\n",
    "    dataframe[\"FLAG_PAST_INTERNATIONAL_PURCHASE\"] = [int(x) for x in (dataframe[\"FLAG_INTERNATIONAL_INTERNET\"]==1) & (dataframe[\"MEAN_INTERNATIONAL_PAST_7DAY\"]>0)]\n",
    "    \n",
    "    dataframe[\"CRDT_TO_AMOUNT\"] = dataframe['AVAIL_CRDT'] / dataframe['AMOUNT']\n",
    "    dataframe[\"AMOUNT_TO_LIMIT\"] = dataframe['AMOUNT'] / (dataframe['CREDIT_LIMIT']+1)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY_COLUMNS += ['FLAG_INTERNATIONAL_INTERNET', 'FLAG_PAST_INTERNATIONAL_PURCHASE']\n",
    "NUMERICAL_COLUMNS += ['AMOUNT_TO_LIMIT', 'EVENT_TIME_IN_SECONDS', 'CRDT_TO_AMOUNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPreprocessed = data_preprocessing(data)\n",
    "dataEvalPreprocessed = data_preprocessing(evalData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# External Data\n",
    "Add any useful external features\n",
    "\n",
    "- 2021 Canada Crime Statistic (https://www150.statcan.gc.ca/n1/en/pub/85-002-x/2022001/article/00013-eng.pdf?st=rc5-elU3)\n",
    "- 2022 COVID-19 Cases (https://health-infobase.canada.ca/covid-19/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the graph into estimated reported crime count\n",
    "extData = pd.DataFrame({'EVENT_MONTH': [2,3,4,5,6,7],\n",
    "              'REPORTED_CRIMINAL_CODE_CRIME_EXCULDING_TRAFFIC': [140000, 160000, 155000, 170000, 178000, 186000]})\n",
    "\n",
    "NUMERICAL_COLUMNS.append('REPORTED_CRIMINAL_CODE_CRIME_EXCULDING_TRAFFIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of covid-19 cases in Canada in 2022 using pandas\n",
    "covidData = pd.read_csv('covid19-data.csv')\n",
    "\n",
    "# Ensure same name for easy join later\n",
    "covidData['EVENT_MONTH'] = pd.to_datetime(covidData['date']).dt.month\n",
    "covidData['year'] = pd.to_datetime(covidData['date']).dt.year\n",
    "\n",
    "covidDataToJoin = covidData[(covidData['prname'] == 'Canada') & (covidData['year'] == 2022)].groupby(['EVENT_MONTH'])['totalcases'].sum()\n",
    "covidDataToJoin.name = 'COVID_TOTAL_CASES'\n",
    "\n",
    "NUMERICAL_COLUMNS.append('COVID_TOTAL_CASES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the dataframes to get new column(s)\n",
    "dataProcessed = dataPreprocessed.merge(extData, on = 'EVENT_MONTH')\n",
    "dataProcessed = dataProcessed.merge(covidDataToJoin, on = 'EVENT_MONTH')\n",
    "\n",
    "dataEvalProcessed = dataEvalPreprocessed.merge(extData, on = 'EVENT_MONTH')\n",
    "dataEvalProcessed = dataEvalProcessed.merge(covidDataToJoin, on = 'EVENT_MONTH')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Determine which features are important via Random Forest and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features to keep values in the same scale and improve accuracy/stabilize  models\n",
    "def normalize(data, normalizerObject, train=True):\n",
    "    \"\"\"Normalize the data depending on the choice of normalizerObjecvt.\n",
    "\n",
    "    Args:\n",
    "        data (DataFrame): pandas DataFrame\n",
    "        normalizerObject (sklearn type of normalizer): ex) StandardScaler\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: normalized data\n",
    "        Normalizer Object: fitted normalizer\n",
    "    \"\"\"\n",
    "    if train:\n",
    "        sc = normalizerObject()\n",
    "        xNumericalNormalized = pd.DataFrame(sc.fit_transform(data), columns = data.columns, index=data.index)\n",
    "    else:\n",
    "        sc = normalizerObject\n",
    "        xNumericalNormalized = pd.DataFrame(sc.transform(data), columns = data.columns, index=data.index)\n",
    "        \n",
    "    return xNumericalNormalized, sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = dataProcessed['FRAUD_FLAG']\n",
    "X = dataProcessed.drop(['FRAUD_FLAG'], axis = 1)\n",
    "xEval = dataEvalProcessed.drop(['FRAUD_FLAG'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "n_features = 100\n",
    "\n",
    "rf = RandomForestClassifier(random_state=0, max_depth=7)\n",
    "variableRank = Counter()\n",
    "\n",
    "for i in range(n_runs):\n",
    "    print(f'Run {i}:')\n",
    "    xRF, _, yRF, _ = train_test_split(X, y, train_size=0.3, stratify=y, random_state=i)\n",
    "    rf.fit(xRF, yRF)\n",
    "    # Greatest to Least\n",
    "    sort = (-rf.feature_importances_).argsort()\n",
    "    for rank, col in enumerate(rf.feature_names_in_[sort]):\n",
    "        variableRank[col] += rank\n",
    "\n",
    "# Return the columns that have the lowest sum of the ranks\n",
    "keepColumns = [x[0] for x in variableRank.most_common()[:(-len(X.columns)-1):-1]][:n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the \"important\" columns\n",
    "xImportant = X[keepColumns]\n",
    "xEvalImportant = xEval[keepColumns]\n",
    "xImportant.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA on the numerical features to reduce the total number of features\n",
    "n_components=50\n",
    "pca = PCA(n_components)\n",
    "\n",
    "numericalFeatures = xImportant.dtypes[xImportant.dtypes == 'float64'].index\n",
    "binaryFeatures = xImportant.dtypes[xImportant.dtypes != 'float64'].index\n",
    "\n",
    "# Normalize using StandardScaler (before PCA)\n",
    "xNumericalNormalized, sc = normalize(xImportant[numericalFeatures], StandardScaler, train=True)\n",
    "xEvalNumericalNormalized, _ = normalize(xEvalImportant[numericalFeatures], sc, train=False)\n",
    "\n",
    "xNormalized = pd.concat([xNumericalNormalized, xImportant[binaryFeatures]], axis=1)\n",
    "xEvalNormalized = pd.concat([xEvalNumericalNormalized, xEvalImportant[binaryFeatures]], axis=1)\n",
    "\n",
    "# Fit and apply PCA transformation\n",
    "xPCA = pca.fit_transform(xNumericalNormalized)\n",
    "xEvalPCA = pca.transform(xEvalNumericalNormalized)\n",
    "\n",
    "xContinuousImportant = pd.DataFrame(xPCA, columns = [f'V{i}'for i in range(n_components)])\n",
    "xEvalContinuousImportant = pd.DataFrame(xEvalPCA, columns = [f'V{i}'for i in range(n_components)])\n",
    "\n",
    "xImportant = pd.concat([xNormalized[binaryFeatures], xContinuousImportant], axis = 1)\n",
    "xEvalImportant = pd.concat([xEvalNormalized[binaryFeatures], xEvalContinuousImportant], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sampling\n",
    "\n",
    "### Synthetic Minority Oversampling TEchniques (SMOTE)\n",
    "\n",
    "#### Description:\n",
    "- randomly select a point in the minority class\n",
    "- calculate the euclidean distance between each point in the minority class and find the $k$ nearest points\n",
    "- randomly select a point among the $k$ points\n",
    "- generate new data point (randomly on the line, the weight of the equation between two points is randomly generated as well)\n",
    "\n",
    "### SMOTE-NC (Nominal Continuous)\n",
    "Extension of SMOTE. In order to include categorical features into the synthetic data generation:\n",
    "- randomly select a point in the minority class\n",
    "- calculate the euclidean distance between each point in the minority class and find the $k$ nearest points\n",
    "    - for each nominal category, substitute in the euclidean distance calculation the MEDIAN of STD of the continous classes\n",
    "- randomly select a point among the $k$ points\n",
    "- generate new data point\n",
    "    - for continous values, same as SMOTE\n",
    "    - for nominal values, take the randomly selected points nominal value\n",
    "\n",
    "### SMOTE-ENC (Encoded Nominal and Continuous)\n",
    "- essentially the same as before, but this time nominal are encoded and have numerical \"weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# SMOTE-ENC Proposed in the paper -- adjusted code since it was out of date and for our use case\n",
    "# --------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from sklearn.utils import check_array, sparsefuncs_fast, _safe_indexing, check_X_y, check_random_state\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from scipy import sparse\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.base import clone\n",
    "from numbers import Integral\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, SVMSMOTE\n",
    "import os\n",
    "# import missingpy as missingpy\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pickle\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, f1_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "class MySMOTENC():\n",
    "    \n",
    "    def __init__(self, categorical_features):\n",
    "        self.categorical_features = categorical_features\n",
    "        \n",
    "    def chk_neighbors(self, nn_object, additional_neighbor):\n",
    "        if isinstance(nn_object, Integral):\n",
    "            return NearestNeighbors(n_neighbors=nn_object + additional_neighbor)\n",
    "        elif isinstance(nn_object, KNeighborsMixin):\n",
    "            return clone(nn_object)\n",
    "        else:\n",
    "            raise_isinstance_error(nn_name, [int, KNeighborsMixin], nn_object)     \n",
    "    \n",
    "    def generate_samples(self, X, nn_data, nn_num, rows, cols, steps, continuous_features_,):\n",
    "        rng = check_random_state(42)\n",
    "\n",
    "        diffs = nn_data[nn_num[rows, cols]] - X[rows]\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            sparse_func = type(X).__name__\n",
    "            steps = getattr(sparse, sparse_func)(steps)\n",
    "            X_new = X[rows] + steps.multiply(diffs)\n",
    "        else:\n",
    "            X_new = X[rows] + steps * diffs \n",
    "\n",
    "        X_new = (X_new.tolil() if sparse.issparse(X_new) else X_new)\n",
    "        # convert to dense array since scipy.sparse doesn't handle 3D\n",
    "        nn_data = (nn_data.toarray() if sparse.issparse(nn_data) else nn_data)\n",
    "\n",
    "        all_neighbors = nn_data[nn_num[rows]]\n",
    "\n",
    "        for idx in range(continuous_features_.size, X.shape[1]):\n",
    "\n",
    "            mode = stats.mode(all_neighbors[:, :, idx], axis = 1)[0]\n",
    "\n",
    "            X_new[:, idx] = np.ravel(mode)            \n",
    "        return X_new\n",
    "    \n",
    "    def make_samples(self, X, y_dtype, y_type, nn_data, nn_num, n_samples, continuous_features_, step_size=1.0):\n",
    "        random_state = check_random_state(42)\n",
    "        samples_indices = random_state.randint(low=0, high=len(nn_num.flatten()), size=n_samples)    \n",
    "        steps = step_size * random_state.uniform(size=n_samples)[:, np.newaxis]\n",
    "        rows = np.floor_divide(samples_indices, nn_num.shape[1])\n",
    "        cols = np.mod(samples_indices, nn_num.shape[1])\n",
    "\n",
    "        X_new = self.generate_samples(X, nn_data, nn_num, rows, cols, steps, continuous_features_)\n",
    "        y_new = np.full(n_samples, fill_value=y_type, dtype=y_dtype)\n",
    "        \n",
    "        return X_new, y_new\n",
    "    \n",
    "    def cat_corr_pandas(self, X, target_df, target_column, target_value):\n",
    "    # X has categorical columns\n",
    "        categorical_columns = list(X.columns)\n",
    "        X = pd.concat([X, target_df], axis=1)\n",
    "\n",
    "        # filter X for target value\n",
    "        is_target = X.loc[:, target_column] == target_value\n",
    "        X_filtered = X.loc[is_target, :]\n",
    "\n",
    "        X_filtered.drop(target_column, axis=1, inplace=True)\n",
    "\n",
    "        # get columns in X\n",
    "        nrows = len(X)\n",
    "        encoded_dict_list = []\n",
    "        nan_dict = dict({})\n",
    "        c = 0\n",
    "        imb_ratio = len(X_filtered)/len(X)\n",
    "        OE_dict = {}\n",
    "        \n",
    "        for column in categorical_columns:\n",
    "            for level in list(X.loc[:, column].unique()):\n",
    "                \n",
    "                # filter rows where level is present\n",
    "                row_level_filter = X.loc[:, column] == level\n",
    "                rows_in_level = len(X.loc[row_level_filter, :])\n",
    "                \n",
    "                # number of rows in level where target is 1\n",
    "                O = len(X.loc[is_target & row_level_filter, :])\n",
    "                E = rows_in_level * imb_ratio\n",
    "                # Encoded value = chi, i.e. (observed - expected)/expected\n",
    "                ENC = (O - E) / E\n",
    "                OE_dict[level] = ENC\n",
    "                \n",
    "            encoded_dict_list.append(OE_dict)\n",
    "\n",
    "            X.loc[:, column] = X[column].map(OE_dict)\n",
    "\n",
    "            nan_idx_array = np.ravel(np.argwhere(np.isnan(X.loc[:, column].to_numpy())))\n",
    "            if len(nan_idx_array) > 0 :\n",
    "                nan_dict[c] = nan_idx_array\n",
    "            c = c + 1\n",
    "            X.loc[:, column].fillna(-1, inplace = True)\n",
    "            \n",
    "        X.drop(target_column, axis=1, inplace=True)\n",
    "        return X, encoded_dict_list, nan_dict\n",
    "\n",
    "    def fit_resample(self, X, y):\n",
    "        X_cat_encoded, encoded_dict_list, nan_dict = self.cat_corr_pandas(X.iloc[:,np.asarray(self.categorical_features)], y, target_column='FRAUD_FLAG', target_value=1)\n",
    "#         X_cat_encoded = np.ravel(np.array(X_cat_encoded))\n",
    "        X_cat_encoded = np.array(X_cat_encoded)\n",
    "        y = np.ravel(y)\n",
    "        X = np.array(X)\n",
    "\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        target_stats = dict(zip(unique, counts))\n",
    "        n_sample_majority = max(target_stats.values())\n",
    "        class_majority = max(target_stats, key=target_stats.get)\n",
    "        # MGW - DEBUG\n",
    "        # sampling_strategy = {key: n_sample_majority - value for (key, value) in target_stats.items() if key != class_majority}\n",
    "        sampling_strategy = {1: 40000}\n",
    "        print(sampling_strategy)\n",
    "        n_features_ = X.shape[1]\n",
    "        categorical_features = np.asarray(self.categorical_features)\n",
    "        if categorical_features.dtype.name == 'bool':\n",
    "            categorical_features_ = np.flatnonzero(categorical_features)\n",
    "        else:\n",
    "            if any([cat not in np.arange(n_features_) for cat in categorical_features]):\n",
    "                raise ValueError('Some of the categorical indices are out of range. Indices'\n",
    "                            ' should be between 0 and {}'.format(n_features_))\n",
    "            categorical_features_ = categorical_features\n",
    "\n",
    "        continuous_features_ = np.setdiff1d(np.arange(n_features_),categorical_features_)\n",
    "\n",
    "        target_stats = Counter(y)\n",
    "        class_minority = min(target_stats, key=target_stats.get)\n",
    "\n",
    "        X_continuous = X[:, continuous_features_]\n",
    "        X_continuous = check_array(X_continuous, accept_sparse=['csr', 'csc'])\n",
    "        X_minority = _safe_indexing(X_continuous, np.flatnonzero(y == class_minority))\n",
    "\n",
    "        if sparse.issparse(X):\n",
    "            if X.format == 'csr':\n",
    "                _, var = sparsefuncs_fast.csr_mean_variance_axis0(X_minority)\n",
    "            else:\n",
    "                _, var = sparsefuncs_fast.csc_mean_variance_axis0(X_minority)\n",
    "        else:\n",
    "            var = X_minority.var(axis=0)\n",
    "        median_std_ = np.median(np.sqrt(var))\n",
    "\n",
    "        X_categorical = X[:, categorical_features_]\n",
    "        X_copy = np.hstack((X_continuous, X_categorical))\n",
    "\n",
    "        X_cat_encoded = X_cat_encoded * median_std_\n",
    "\n",
    "        X_encoded = np.hstack((X_continuous, X_cat_encoded))\n",
    "        X_resampled = X_encoded.copy()\n",
    "        y_resampled = y.copy()\n",
    "\n",
    "\n",
    "        for class_sample, n_samples in sampling_strategy.items():\n",
    "            if n_samples == 0:\n",
    "                continue\n",
    "            target_class_indices = np.flatnonzero(y == class_sample)\n",
    "            X_class = _safe_indexing(X_encoded, target_class_indices)\n",
    "            nn_k_ = self.chk_neighbors(5, 1)\n",
    "            nn_k_.fit(X_class)\n",
    "\n",
    "            nns = nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
    "            X_new, y_new = self.make_samples(X_class, y.dtype, class_sample, X_class, nns, n_samples, continuous_features_, 1.0)\n",
    "\n",
    "            if sparse.issparse(X_new):\n",
    "                X_resampled = sparse.vstack([X_resampled, X_new])\n",
    "                sparse_func = 'tocsc' if X.format == 'csc' else 'tocsr'\n",
    "                X_resampled = getattr(X_resampled, sparse_func)()\n",
    "            else:\n",
    "                X_resampled = np.vstack((X_resampled, X_new))\n",
    "            y_resampled = np.hstack((y_resampled, y_new))\n",
    "            \n",
    "        X_resampled_copy = X_resampled.copy()\n",
    "        i = 0\n",
    "        for col in range(continuous_features_.size, X.shape[1]):\n",
    "            encoded_dict = encoded_dict_list[i]\n",
    "            i = i + 1\n",
    "            for key, value in encoded_dict.items():\n",
    "                X_resampled_copy[:, col] = np.where(np.round(X_resampled_copy[:, col], 4) == np.round(value * median_std_, 4), key, X_resampled_copy[:, col])\n",
    "\n",
    "        for key, value in nan_dict.items():\n",
    "            for item in value:\n",
    "                X_resampled_copy[item, continuous_features_.size + key] = X_copy[item, continuous_features_.size + key]\n",
    "\n",
    "               \n",
    "        X_resampled = X_resampled_copy   \n",
    "        indices_reordered = np.argsort(np.hstack((continuous_features_, categorical_features_)))\n",
    "        if sparse.issparse(X_resampled):\n",
    "            col_indices = X_resampled.indices.copy()\n",
    "            for idx, col_idx in enumerate(indices_reordered):\n",
    "                mask = X_resampled.indices == col_idx\n",
    "                col_indices[mask] = idx\n",
    "            X_resampled.indices = col_indices\n",
    "        else:\n",
    "            X_resampled = X_resampled[:, indices_reordered]\n",
    "        return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSampling(X, y, methods):\n",
    "    \"\"\" Sample the data set in order of method in methods.\n",
    "\n",
    "    Args:\n",
    "        X (DataFrame): covariates\n",
    "        y (DataFrame): predictor\n",
    "        methods (list): list of methods in order\n",
    "\n",
    "    Returns:\n",
    "        (DataFrame, DataFrame): newly sampled dataset\n",
    "    \"\"\"\n",
    "    xNew, yNew = X, y\n",
    "    for method in methods:\n",
    "        print(method)\n",
    "        print(f\"Original: \\n{(yNew == 1).sum()}, {(yNew == 0).sum()}\")\n",
    "        if (method == 'SMOTE') and np.any(np.where(xNew.dtypes == 'int64')[0]):\n",
    "            sampler = SMOTENC(categorical_features=np.where(xNew.dtypes == 'int64')[0], sampling_strategy=0.2, random_state=0)\n",
    "        elif method == 'SMOTENC':\n",
    "            sampler = SMOTENC(categorical_features=np.where(xNew.dtypes == 'int64')[0], sampling_strategy=0.2, random_state=0)\n",
    "        elif method == 'SMOTEENC':\n",
    "            sampler = MySMOTENC(categorical_features=np.where(xNew.dtypes == 'int64')[0])\n",
    "        elif method == 'OVER_SAMPLE':\n",
    "            sampler = RandomOverSampler(sampling_strategy=0.5, random_state=0)\n",
    "        elif method == 'UNDER_SAMPLE':\n",
    "            sampler = RandomUnderSampler(sampling_strategy=0.8)\n",
    "        xNew, yNew = sampler.fit_resample(xNew, yNew)\n",
    "        print(f\"New: \\n{(yNew == 1).sum()}, {(yNew == 0).sum()}\")\n",
    "        print('---')\n",
    "        \n",
    "        if isinstance(xNew, type(np.array([]))):\n",
    "            xNew = pd.DataFrame(xNew, columns = X.columns)\n",
    "        if isinstance(yNew, type(np.array([]))):\n",
    "            yNew = pd.Series(yNew, name = 'FRAUD_FLAG')\n",
    "        \n",
    "    \n",
    "    return xNew, yNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset to test accuracy on unseen data\n",
    "xTrainDS, xTestDS, yTrainDS, yTestDS = train_test_split(xImportant, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Determine the methods to apply (IN ORDER)\n",
    "methods = ['SMOTEENC', 'UNDER_SAMPLE']\n",
    "xTrainSampled, yTrainSampled = dataSampling(xTrainDS, yTrainDS, methods=methods)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Step\n",
    "Define helper functions for easy modeling and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    \"\"\" Evaluate the model. Return useful metrics such as:\n",
    "            - confusion matrix\n",
    "            - f1 score\n",
    "            - percision\n",
    "            - recall\n",
    "\n",
    "    Args:\n",
    "        y_true (Series): the true labels\n",
    "        y_pred (Series): the predicted labels\n",
    "\n",
    "    \"\"\"\n",
    "    # C_(i,j) = group i predicted to be in group j\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    percision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Percision: {percision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.title('Confusion Matrix')\n",
    "    sns.heatmap(cm, annot=True, fmt='.5g')\n",
    "    return cm, f1, percision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(xTrain, xTest, yTrain, yTest, model):\n",
    "    \"\"\" Given the model, train and evaluate.\n",
    "\n",
    "    Args:\n",
    "        xTrain (DataFrame): training sets covariates\n",
    "        xTest (DataFrame): test sets covariates\n",
    "        yTrain (Series): training set labels\n",
    "        yTest (Series): test set labels\n",
    "        model (Model Type): any sklearn model (xgboost works as well)\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(xTrain, yTrain)\n",
    "    yPred = model.predict(xTest)\n",
    "    \n",
    "    # Evaluation\n",
    "    cm, f1, percision, recall = evaluate(yTest, yPred)\n",
    "    probs = model.predict_proba(xTest)[:,1]\n",
    "    AUC = roc_auc_score(yTest, probs)\n",
    "    print(f'AUC: {AUC}')\n",
    "    return model, cm, AUC, f1, percision, recall, xTrain, xTest, yTrain, yTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xFinal, yFinal = xTrainSampled, yTrainSampled\n",
    "xTestFinal, yTestFinal = xTestDS, yTestDS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "Test different state-of-the-art supervised binary classification models and select the \"best\" one.\n",
    "- Logistic\n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logisitic Regression\n",
    "modelLR, cm, AUC, f1, percision, recall, xTrain, xTest, yTrain, yTest = modeling(xFinal, xTestFinal, yFinal, yTestFinal, LogisticRegression(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "modelDT, cm, AUC, f1, percision, recall, xTrain, xTest, yTrain, yTest = modeling(xFinal, xTestFinal, yFinal, yTestFinal, DecisionTreeClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "modelRF, cm, AUC, f1, percision, recall, xTrain, xTest, yTrain, yTest = modeling(xFinal, xTestFinal, yFinal, yTestFinal, RandomForestClassifier(random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "modelXGB, cm, AUC, f1, percision, recall, xTrain, xTest, yTrain, yTest = modeling(xFinal, xTestFinal, yFinal, yTestFinal, xgb.XGBClassifier(random_state=0, scale_pos_weight=0.1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Data to Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yEvalPred = modelXGB.predict(xEvalImportant)\n",
    "\n",
    "yEvalProb = modelXGB.predict_proba(xEvalImportant)\n",
    "yEvalProbFraud = yEvalProb[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xToSubmit = pd.concat([evalData['TRANSACTION_ID'], \n",
    "                       pd.Series(yEvalPred, name='PREDICTION'), \n",
    "                       pd.Series(yEvalProbFraud, name='PROBABILITY')], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xToSubmit.to_csv(\"KO-WOO-REANS_prediction.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scotiabank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a9bc83a6c1ff07071eeba09d414ddd4a4d18bd9f8f48b25cb05048b92844e59"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
